{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'2.0.0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. preprocessing data\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# 去西班牙语重音\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "en_sentence = 'No way!'\n",
    "spa_sentence = '¡De ningún modo!'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'No way!'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_to_ascii(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'¡De ningun modo!'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_to_ascii(spa_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # 标点符号前后加空格\n",
    "    s = re.sub(r'([?.!,¿])', r'\\1', s)\n",
    "    # 多余的空格变成一个空格\n",
    "    s = re.sub(r'[\" \"]+', ' ', s)\n",
    "    # 除标点符号和字母外都是空格\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]', ' ', s)\n",
    "    # 前后去空格\n",
    "    s = s.rstrip().strip()\n",
    "\n",
    "    return '<start>' + s + '<end>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'<start>no way!<end>'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(en_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'<start>de ningun modo!<end>'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(spa_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    sentence_pairs = [line.split('\\t') for line in lines]\n",
    "    preprocessed_sentence_pairs = [\n",
    "         (preprocess_sentence(en), preprocess_sentence(sp)) for en,sp in sentence_pairs\n",
    "    ]\n",
    "    return zip(*preprocessed_sentence_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(1, 3, 5) (2, 4, 6)\n"
    }
   ],
   "source": [
    "a = [(1, 2), (3, 4), (5, 6)]\n",
    "c, d = zip(*a)\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dataset,sp_dataset =  parse_data('spa.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'<start>if you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.<end>'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'<start>si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.<end>'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "12 7\n"
    }
   ],
   "source": [
    "def tokenizer(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None, filters='', split=' '\n",
    "    )\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[:30000])\n",
    "\n",
    "max_length_input = max(len(t) for t in input_tensor)\n",
    "max_length_output = max(len(t) for t in output_tensor)\n",
    "\n",
    "print(max_length_input, max_length_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "24000 24000\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_train, input_val, output_train, output_val = train_test_split(\n",
    "    input_tensor, output_tensor, test_size = 0.2\n",
    ")\n",
    "\n",
    "print(len(input_train), len(output_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(64, 12) (64, 7)\ntf.Tensor(\n[[   65  3939   211     0     0     0     0     0     0     0     0     0]\n [  192    10  3325     0     0     0     0     0     0     0     0     0]\n [  228   280  1014     0     0     0     0     0     0     0     0     0]\n [  248     8  5031     0     0     0     0     0     0     0     0     0]\n [   13     9  2624     0     0     0     0     0     0     0     0     0]\n [    4    48   936     0     0     0     0     0     0     0     0     0]\n [   64     8  4280    60  1137     0     0     0     0     0     0     0]\n [    7     2  2236     0     0     0     0     0     0     0     0     0]\n [   43  3176   231  2947     0     0     0     0     0     0     0     0]\n [    7  1582     2   944     0     0     0     0     0     0     0     0]\n [    7   142   633     0     0     0     0     0     0     0     0     0]\n [   95    12  4947     0     0     0     0     0     0     0     0     0]\n [   28   116   713     0     0     0     0     0     0     0     0     0]\n [ 4154   733 11705     0     0     0     0     0     0     0     0     0]\n [   20    30  1393     0     0     0     0     0     0     0     0     0]\n [ 3305  7438     6  2264     0     0     0     0     0     0     0     0]\n [ 2279     9   983     0     0     0     0     0     0     0     0     0]\n [    1    18   921   153     0     0     0     0     0     0     0     0]\n [    7    12    10   262     0     0     0     0     0     0     0     0]\n [ 7292     0     0     0     0     0     0     0     0     0     0     0]\n [   83    16   798     0     0     0     0     0     0     0     0     0]\n [  433    51  1073   293     0     0     0     0     0     0     0     0]\n [ 2680  2141     0     0     0     0     0     0     0     0     0     0]\n [  435   206   166     0     0     0     0     0     0     0     0     0]\n [    4   164  5824     0     0     0     0     0     0     0     0     0]\n [ 1406     2    32   150     0     0     0     0     0     0     0     0]\n [  109  8536     0     0     0     0     0     0     0     0     0     0]\n [  426   246 10276     0     0     0     0     0     0     0     0     0]\n [  307    25    69     0     0     0     0     0     0     0     0     0]\n [  452     9   985     0     0     0     0     0     0     0     0     0]\n [   34  1409    10  1271     0     0     0     0     0     0     0     0]\n [    1     2     5   119   901     0     0     0     0     0     0     0]\n [   13   889   116  4043     0     0     0     0     0     0     0     0]\n [ 1510   417  4283     0     0     0     0     0     0     0     0     0]\n [    7    27  2581     3    33   408     0     0     0     0     0     0]\n [    7  3022    12   810     0     0     0     0     0     0     0     0]\n [    4  1382   120     0     0     0     0     0     0     0     0     0]\n [ 7349     0     0     0     0     0     0     0     0     0     0     0]\n [   20    18    99    16  1072     0     0     0     0     0     0     0]\n [ 2632  1053    25    78     0     0     0     0     0     0     0     0]\n [  433   914    51     6    22     0     0     0     0     0     0     0]\n [ 2354   899     0     0     0     0     0     0     0     0     0     0]\n [  907   580  1990     0     0     0     0     0     0     0     0     0]\n [    1    12     2 10209     0     0     0     0     0     0     0     0]\n [  222    72     0     0     0     0     0     0     0     0     0     0]\n [   24    15  4838     0     0     0     0     0     0     0     0     0]\n [   64     8    12  5563     0     0     0     0     0     0     0     0]\n [  194  4607     0     0     0     0     0     0     0     0     0     0]\n [ 1172     3    19  8560     0     0     0     0     0     0     0     0]\n [   86  2022   145     0     0     0     0     0     0     0     0     0]\n [    7     9  3854     0     0     0     0     0     0     0     0     0]\n [   23  4894     0     0     0     0     0     0     0     0     0     0]\n [14297  3190     0     0     0     0     0     0     0     0     0     0]\n [  177  6646     0     0     0     0     0     0     0     0     0     0]\n [   13   110    33  7879     0     0     0     0     0     0     0     0]\n [   39   874    17   357     8  5440     0     0     0     0     0     0]\n [ 1829 13515     0     0     0     0     0     0     0     0     0     0]\n [   21    18   724  7862     0     0     0     0     0     0     0     0]\n [ 1620    14    10  3218     0     0     0     0     0     0     0     0]\n [ 3055  1305     0     0     0     0     0     0     0     0     0     0]\n [ 1515    11   341     0     0     0     0     0     0     0     0     0]\n [  436    18  1841  1613    35   214     0     0     0     0     0     0]\n [   86   918    16  9382     0     0     0     0     0     0     0     0]\n [    4    17  2043     0     0     0     0     0     0     0     0     0]], shape=(64, 12), dtype=int32)\ntf.Tensor(\n[[   2  717  109    0    0    0    0]\n [  46   32 2699    0    0    0    0]\n [  14   21  551    0    0    0    0]\n [ 327   22  713    0    0    0    0]\n [   1  124   35    0    0    0    0]\n [   1  155    6 1664    0    0    0]\n [  83   25    7   88 1400    0    0]\n [   8    3 2263    0    0    0    0]\n [  26  292   90  175    0    0    0]\n [  30 1600    3  837    0    0    0]\n [   8   21 1050    0    0    0    0]\n [1328   56    6   43  186    0    0]\n [   1   44  296   22  370    0    0]\n [ 145 3296    4  575    0    0    0]\n [  18  851   29    0    0    0    0]\n [ 472 4950    0    0    0    0    0]\n [ 541   20  728    0    0    0    0]\n [   2  305   39   53  131    0    0]\n [   8    5   24  284    0    0    0]\n [  12 1063    0    0    0    0    0]\n [   1   61   74  662    0    0    0]\n [ 295   25  178  348    0    0    0]\n [  36    7  179 1864    0    0    0]\n [   1  215   59  101    0    0    0]\n [  10   21    6  261   35    0    0]\n [  33    3   20  170    0    0    0]\n [  10   15  711    0    0    0    0]\n [  14   62  164  870    0    0    0]\n [ 385   53   77    0    0    0    0]\n [ 712   11  964    0    0    0    0]\n [  30 1806    5  251    0    0    0]\n [   2    3    4   92  686    0    0]\n [   8  305   39  296   27 1636    0]\n [  19   62  164 2005    0    0    0]\n [   8    3  391   54  446    0    0]\n [3041  176    6 1188    0    0    0]\n [  26 2068  347    0    0    0    0]\n [  19 1342    0    0    0    0    0]\n [  18  113   39    4  488    0    0]\n [ 924 1317  157    0    0    0    0]\n [ 295  805   25   32  828    0    0]\n [   1  183  692  565    0    0    0]\n [  66    7 6902 1004    0    0    0]\n [   2   99    6 5970    0    0    0]\n [ 423   87    0    0    0    0    0]\n [   1   13   49 3614    0    0    0]\n [  83   82    6    7  325    0    0]\n [  66    7 3506    0    0    0    0]\n [ 282   17 1274    0    0    0    0]\n [ 172   27  404  177    0    0    0]\n [   8 2072  126    0    0    0    0]\n [  26    3  217    0    0    0    0]\n [  19 2667 3404    0    0    0    0]\n [ 185   80    7 3326    0    0    0]\n [   1   31 5055    0    0    0    0]\n [   1   34  244   17  349    0    0]\n [  36    7   23 4711    0    0    0]\n [   1  213 5047    0    0    0    0]\n [1318    5 2638    0    0    0    0]\n [  36    7  463 1950    0    0    0]\n [ 282   39   49  417    0    0    0]\n [   1  316   27  183  339  126    0]\n [ 172   27  405    4 5596    0    0]\n [  42    6 2446   63    0    0    0]], shape=(64, 7), dtype=int32)\n"
    }
   ],
   "source": [
    "def make_dataset(input_tensor, output_tensor, batch_size, epochs=1, shuffle=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (input_tensor, output_tensor)\n",
    "    )\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "train_dataset = make_dataset(\n",
    "    input_train, output_train, batch_size,epochs, True\n",
    ")\n",
    "\n",
    "val_dataset = make_dataset(\n",
    "    input_val, output_val, batch_size\n",
    ")\n",
    "\n",
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape, y.shape)\n",
    "    print(x)\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_units = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(64, 1024) (64, 12, 1024)\n"
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.embedding_units = embedding_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_units)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            encoding_units, return_sequences=True, return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "            )\n",
    "        \n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(sample_hidden.shape, sample_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"encoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        multiple                  3715840   \n_________________________________________________________________\ngru (GRU)                    multiple                  3938304   \n=================================================================\nTotal params: 7,654,144\nTrainable params: 7,654,144\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__\n",
    "        self.mha1 = tf.keras.layers.mu\n",
    "\n",
    "    def call(self,x, encoding_outputs, training, look_ahead_mask, padding_mask):\n",
    "        pass"
   ]
  }
 ]
}